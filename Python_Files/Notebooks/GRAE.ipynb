{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRAE tests\n",
    "-> Implemented manually as the import has dependency flaws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'skbio'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mskbio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistance\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mantel\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Datasets import\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'skbio'"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import shutil\n",
    "import copy\n",
    "from six.moves import cPickle as pickle #for performance\n",
    "\n",
    "import warnings  # Ignore sklearn future warning\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import math\n",
    "import scipy\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import FastICA, PCA\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skbio.stats.distance import mantel\n",
    "\n",
    "# Datasets import\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import torchvision.datasets as torch_datasets\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy import ndimage\n",
    "from scipy.stats import pearsonr\n",
    "import scprep\n",
    "\n",
    "\n",
    "import urllib\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# Model imports\n",
    "import umap\n",
    "import phate\n",
    "\n",
    "# Plots\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment\n",
    "# Experiment parameters\n",
    "RUNS = 10\n",
    "FIT_RATIO = .8\n",
    "DATASETS = ['Embryoid']\n",
    "MODELS = ['AE','GRAE_10', 'GRAE_50', 'GRAE_100', 'Umap_t']\n",
    "\n",
    "# Model parameters\n",
    "BATCH = 200\n",
    "LR = .0001\n",
    "WEIGHT_DECAY = 1\n",
    "EPOCHS = 200\n",
    "\n",
    "# Dataset parameters\n",
    "FIT_DEFAULT = .8  # Ratio of data to use for training\n",
    "SAMPLE = 10000  # Number of points to sample from synthetic manifolds\n",
    "EB_COMPONENTS = 500  # Number of principal components to use of the EB differentiation dataset\n",
    "\n",
    "# Seeds and base path for data\n",
    "BASEPATH = '/yunity/arusty/Graph-Manifold-Alignment/Results/Grae/data'\n",
    "SEED = 7512183 # Used for data generation and splits\n",
    "\n",
    "# 20 Random states for different training runs. Add more if you need more runs\n",
    "RANDOM_STATES = [36087, 63286, 52270, 10387, 40556, 52487, 26512, 28571, 33380,\n",
    "                9369, 28478,  4624, 29114, 41915,  6467,  4216, 16025, 34823,\n",
    "                29854, 23853]\n",
    "\n",
    "# Set seed for both torch and numpy\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Create directory for data\n",
    "if not os.path.exists(BASEPATH):\n",
    "  os.mkdir(BASEPATH)\n",
    "\n",
    "# Create directory for results\n",
    "if not os.path.exists(BASEPATH[:-4] + \"results\"):\n",
    "  os.mkdir(BASEPATH[:-4] + \"results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "# Results Logger\n",
    "class Book():\n",
    "  def __init__(self, datasets, models, metrics):\n",
    "    self.col = ['model', 'dataset', 'run', 'split'] + metrics\n",
    "    self.log = list()\n",
    "    self.models = models\n",
    "    self.datasets = datasets\n",
    "    self.splits = ('train', 'test')\n",
    "    self.metrics = metrics\n",
    "\n",
    "  def add_entry(self, model, dataset, run, split, **kwargs):\n",
    "    # Proof read entry\n",
    "    self.check(model, dataset, split)\n",
    "    self.check_metrics(kwargs)\n",
    "\n",
    "    metrics_ordered = [kwargs[k] for k in self.metrics]\n",
    "\n",
    "    signature = [model, dataset, run, split]\n",
    "    entry = signature +  metrics_ordered\n",
    "\n",
    "    if len(entry) != len(self.col):\n",
    "      raise Exception('Entry size is wrong.')\n",
    "\n",
    "    self.log.append(entry)\n",
    "\n",
    "  def check(self, model, dataset, split):\n",
    "    if model not in self.models:\n",
    "      raise Exception('Invalid model name.')\n",
    "\n",
    "    if dataset not in self.datasets:\n",
    "      raise Exception('Invalid dataset name.')\n",
    "\n",
    "    if split not in self.splits:\n",
    "      raise Exception('Invalid split name.')\n",
    "\n",
    "  def check_metrics(self, kwargs):\n",
    "    if len(kwargs.keys()) != len(self.metrics):\n",
    "      raise Exception('Wrong number of metrics.')\n",
    "\n",
    "    for key in kwargs.keys():\n",
    "      if key not in self.metrics:\n",
    "        raise Exception(f'Trying to add undeclared metric {key}')\n",
    "\n",
    "\n",
    "  def get_df(self):\n",
    "    return pd.DataFrame.from_records(self.log, columns=self.col)\n",
    "\n",
    "\n",
    "def refine_df(df, df_metrics):\n",
    "  df_group = df.groupby(['split', 'dataset', 'model'])\n",
    "  mean = df_group.mean().drop(columns=['run']).round(4)\n",
    "\n",
    "  # Add rank columns\n",
    "  for m in df_metrics:\n",
    "    # Higher is better\n",
    "    ascending = False\n",
    "\n",
    "    if m == 'reconstruction' or m.split('_')[0] == 'mrre':\n",
    "      # Lower is better\n",
    "      ascending = True\n",
    "\n",
    "    loc = mean.columns.get_loc(m) + 1\n",
    "    rank = mean.groupby(['split', 'dataset'])[m].rank(method='min', ascending=ascending)\n",
    "    mean.insert(loc=loc, column=f'{m}_rank', value = rank)\n",
    "\n",
    "  return mean\n",
    "\n",
    "\n",
    "def save_dict(di_, filename_):\n",
    "    with open(filename_, 'wb') as f:\n",
    "        pickle.dump(di_, f)\n",
    "\n",
    "def load_dict(filename_):\n",
    "    with open(filename_, 'rb') as f:\n",
    "        ret_di = pickle.load(f)\n",
    "    return ret_di\n",
    "\n",
    "def slice_3D(X, Y, idx, axis, p=1):\n",
    "  axis = X[:, axis]\n",
    "\n",
    "  sli = np.zeros(shape=X.shape[0])\n",
    "  sli[idx] = 1\n",
    "\n",
    "  sampler = np.random.choice(a=[False, True], size=(sli.shape[0],), p=[1-p, p])\n",
    "\n",
    "  sli = np.logical_and(sli, sampler)\n",
    "\n",
    "  rest = np.logical_not(sli)\n",
    "\n",
    "  X_2, Y_2 = X[rest], Y[rest]\n",
    "  X_3, Y_3 = X[sli], Y[sli]\n",
    "\n",
    "\n",
    "  return X_2, Y_2, X_3, Y_3\n",
    "\n",
    "def make_holes(x, y, n=12, eps_range=(.2, .5), seed=SEED):\n",
    "  np.random.seed(SEED)\n",
    "\n",
    "  hole_idx = np.random.choice(x.shape[0], size=n, replace=False)\n",
    "  d = squareform(pdist(x))\n",
    "  eps_list = np.random.uniform(eps_range[0], eps_range[1], n)\n",
    "\n",
    "  test_idx = list()\n",
    "\n",
    "  for i, idx in enumerate(hole_idx):\n",
    "    d_line, eps = d[idx], eps_list[i]\n",
    "    test_idx.append(np.argwhere(d_line < eps))\n",
    "\n",
    "  test_idx = np.unique(np.concatenate(test_idx))\n",
    "  train_idx = np.full(fill_value=True, shape=x.shape[0])\n",
    "  train_idx[test_idx] = False\n",
    "\n",
    "  return x[train_idx], y[train_idx], x[test_idx], y[test_idx]\n",
    "\n",
    "\n",
    "def plot_3D(x, y, z, c):\n",
    "  fig, (a1)  = plt.subplots(1,1, figsize=(10, 10))\n",
    "  a1 = fig.add_subplot(111, projection='3d')\n",
    "  a1.scatter(x, y, z, c=c, cmap='jet', s = 2)\n",
    "  a1.view_init(elev=10, azim = 90)\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "def plot_3D_grey(x_train, y_train, z_train, x_test, y_test, z_test, c):\n",
    "  fig, (a1)  = plt.subplots(1,1, figsize=(10, 10))\n",
    "  a1 = fig.add_subplot(111, projection='3d')\n",
    "  a1.scatter(x_train, y_train, z_train, color='grey', s = 2, alpha=.5)\n",
    "  a1.scatter(x_test, y_test, z_test, c=c, cmap='jet', s = 2)\n",
    "  a1.view_init(elev=10, azim = 90)\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Vanilla AE\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mConcatDataset\u001b[39;00m(torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mdatasets):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# Vanilla AE\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class ConcatDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, *datasets):\n",
    "        self.datasets = datasets\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return tuple(d[i] for d in self.datasets)\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(d) for d in self.datasets)\n",
    "\n",
    "\n",
    "\n",
    "# AE building blocks\n",
    "class Encoder_MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, hidden_dim3, z_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.linear2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.linear3 = nn.Linear(hidden_dim2, hidden_dim3)\n",
    "        self.mu = nn.Linear(hidden_dim3, z_dim)\n",
    "    def forward(self, x):\n",
    "        hidden1 = F.relu(self.linear(x))\n",
    "        hidden2 = F.relu(self.linear2(hidden1))\n",
    "        hidden3 = F.relu(self.linear3(hidden2))\n",
    "        z_mu = self.mu(hidden3)\n",
    "        return z_mu\n",
    "\n",
    "class Decoder_MLP(nn.Module):\n",
    "    def __init__(self, z_dim, hidden_dim1, hidden_dim2, hidden_dim3, output_dim, sigmoid_act = False):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(z_dim, hidden_dim1)\n",
    "        self.linear2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.linear3 = nn.Linear(hidden_dim2, hidden_dim3)\n",
    "        self.out = nn.Linear(hidden_dim3, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid_act = sigmoid_act\n",
    "    def forward(self, x):\n",
    "        hidden1 = F.relu(self.linear(x))\n",
    "        hidden2 = F.relu(self.linear2(hidden1))\n",
    "        hidden3 = F.relu(self.linear3(hidden2))\n",
    "        if self.sigmoid_act == False:\n",
    "            predicted = (self.out(hidden3))\n",
    "        else:\n",
    "            predicted = F.sigmoid(self.out(hidden3))\n",
    "        return predicted\n",
    "\n",
    "class AE_MLP(nn.Module):\n",
    "    def __init__(self, enc, dec):\n",
    "        super().__init__()\n",
    "        self.enc = enc\n",
    "        self.dec = dec\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent = self.enc(x)\n",
    "        predicted = self.dec(latent)\n",
    "        return predicted, latent\n",
    "\n",
    "\n",
    "# AE main class\n",
    "class AE():\n",
    "    \"\"\"Autoencoder class with sklearn interface.\"\"\"\n",
    "    def __init__(self, input_size, random_state=SEED, track_rec=False,\n",
    "                 AE_wrapper=AE_MLP, batch_size=BATCH, lr=LR,\n",
    "                 weight_decay=WEIGHT_DECAY, reduction='sum', epochs=EPOCHS, **kwargs):\n",
    "      layer_1 = 800\n",
    "      layer_2 = 400\n",
    "      layer_3 = 200\n",
    "      self.lr = lr\n",
    "      self.epochs = epochs\n",
    "      self.batch_size = batch_size\n",
    "      self.weight_decay = weight_decay\n",
    "      self.encoder = Encoder_MLP(input_size, layer_1, layer_2, layer_3, 2)\n",
    "      self.decoder = Decoder_MLP(2, layer_3, layer_2, layer_1, input_size)\n",
    "      self.model = AE_wrapper(self.encoder, self.decoder, **kwargs)\n",
    "      self.model = self.model.float().to(device)\n",
    "\n",
    "      self.criterion = nn.MSELoss(reduction=reduction)\n",
    "\n",
    "      self.optimizer = torch.optim.Adam(self.model.parameters(),\n",
    "                                        lr = self.lr,\n",
    "                                        weight_decay=self.weight_decay)\n",
    "      self.loss = list()\n",
    "      self.track_rec = track_rec\n",
    "      self.random_state = random_state\n",
    "\n",
    "    def fit(self, x):\n",
    "      # Train AE\n",
    "      self.model.train()\n",
    "\n",
    "      # Reproducibility\n",
    "      torch.manual_seed(self.random_state)\n",
    "      torch.backends.cudnn.deterministic = True\n",
    "      torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "      loader = torch.utils.data.DataLoader(x, batch_size=self.batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "      for epoch in range(self.epochs):\n",
    "        for batch in loader:\n",
    "            data, y = batch\n",
    "            data = data.to(device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            x_hat, _ = self.model(data)\n",
    "            x_hat = x_hat.to(device)\n",
    "            loss = self.criterion(data, x_hat)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        if self.track_rec:\n",
    "          x_np, _ = x.numpy()\n",
    "          x_hat = self.inverse_transform(self.transform(x))\n",
    "          self.loss.append(mean_squared_error(x_np, x_hat))\n",
    "\n",
    "    def transform(self, x):\n",
    "      self.model.eval()\n",
    "      loader = torch.utils.data.DataLoader(x, batch_size=self.batch_size,\n",
    "                                           shuffle=False)\n",
    "      z = [self.encoder(batch.to(device)).cpu().detach().numpy()\n",
    "      for batch, _ in loader]\n",
    "\n",
    "      return np.concatenate(z)\n",
    "\n",
    "    def fit_transform(self, x):\n",
    "      self.fit(x)\n",
    "      return self.transform(x)\n",
    "\n",
    "    def inverse_transform(self, z):\n",
    "      self.model.eval()\n",
    "      z = NumpyDataset(z)\n",
    "      loader = torch.utils.data.DataLoader(z, batch_size=self.batch_size,\n",
    "                                           shuffle=False)\n",
    "      x_hat = [self.decoder(batch.to(device)).cpu().detach().numpy()\n",
    "      for batch in loader]\n",
    "\n",
    "      return np.concatenate(x_hat)\n",
    "\n",
    "\n",
    "class ManifoldLoss(nn.Module):\n",
    "    def __init__(self, lam):\n",
    "      super().__init__()\n",
    "      self.lam = lam\n",
    "      self.MSE = nn.MSELoss(reduction='sum')\n",
    "      self.loss = None\n",
    "\n",
    "    def forward(self, x, y, z, emb):\n",
    "      self.loss = self.MSE(x, y) + self.lam * self.MSE(z, emb)\n",
    "      return self.loss\n",
    "\n",
    "    def backward(self):\n",
    "      self.loss.backward()\n",
    "\n",
    "    def decay_lam(self, factor):\n",
    "      self.lam *= factor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grae defintion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ManifoldNet(AE):\n",
    "  \"\"\"Base class for GRAE.\"\"\"\n",
    "  def __init__(self, input_size, embedder, random_state, track_rec=False, lam=10, lam_decay=1, **kwargs):\n",
    "      super().__init__(input_size, random_state, track_rec)\n",
    "      self.criterion = ManifoldLoss(lam)\n",
    "      self.emb = None\n",
    "      self.targets = None\n",
    "      self.precomputed = False\n",
    "      self.embedder_args = kwargs\n",
    "      self.embedder = embedder\n",
    "      self.lam_decay = lam_decay\n",
    "\n",
    "  def fit(self, x):\n",
    "    self.model.train()\n",
    "\n",
    "    # Reproducibility\n",
    "    torch.manual_seed(self.random_state)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    x_np, self.targets = x.numpy()\n",
    "\n",
    "    # Compute embedding target\n",
    "    if not self.precomputed:\n",
    "      embedder_m = self.embedder(**self.embedder_args,\n",
    "                                 random_state=self.random_state)\n",
    "\n",
    "      emb = embedder_m.fit_transform(x_np)\n",
    "\n",
    "      # Normalize\n",
    "      emb = scipy.stats.zscore(emb)\n",
    "\n",
    "      self.emb = emb\n",
    "\n",
    "\n",
    "    # Loader to iterate over both data batches and target embedding batches\n",
    "    data = ConcatDataset(torch.from_numpy(self.emb).float(), x)\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(data,\n",
    "                                          batch_size=self.batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "    for epoch in range(self.epochs):\n",
    "      for embedding, batch in loader:\n",
    "          x_in, _ = batch[0].to(device), batch[1]\n",
    "          embedding = embedding.to(device)\n",
    "          self.optimizer.zero_grad()\n",
    "          x_hat, z_mu = self.model(x_in)\n",
    "\n",
    "          x_hat = x_hat.to(device)\n",
    "          z_mu = z_mu.to(device)\n",
    "          self.criterion(x_in, x_hat, z_mu, embedding)\n",
    "          self.criterion.backward()\n",
    "          self.optimizer.step()\n",
    "\n",
    "      self.criterion.decay_lam(self.lam_decay)\n",
    "\n",
    "      if self.track_rec:\n",
    "        x_hat = self.inverse_transform(self.transform(x))\n",
    "        self.loss.append(mean_squared_error(x_np, x_hat))\n",
    "\n",
    "  def plot_latent(self):\n",
    "    plt.scatter(*self.emb.T, c=self.targets, cmap=\"jet\")\n",
    "    plt.show()\n",
    "\n",
    "  def set_embedding(self, emb):\n",
    "    self.emb = emb\n",
    "    self.precomputed = True\n",
    "\n",
    "\n",
    "\n",
    "# Variants of GRAE\n",
    "class GRAE(ManifoldNet):\n",
    "  \"\"\"Vanilla GRAE.\"\"\"\n",
    "  def __init__(self, input_size, random_state=SEED, track_rec=False,\n",
    "               lam=10, lam_decay=1, t='auto', knn=20, n_landmark=2000,\n",
    "               mds='metric'):\n",
    "    super().__init__(input_size=input_size,\n",
    "                      random_state=random_state,\n",
    "                      embedder=phate.PHATE,\n",
    "                      track_rec=track_rec,\n",
    "                      lam=lam,\n",
    "                      lam_decay=lam_decay,\n",
    "                      t=t,\n",
    "                      knn=knn,\n",
    "                      n_jobs=-1,\n",
    "                      verbose=0,\n",
    "                      n_landmark=n_landmark,\n",
    "                      mds=mds)\n",
    "\n",
    "\n",
    "class GRAE_UMAP(ManifoldNet):\n",
    "  \"\"\"UMAP GRAE, as presented in the supplement.\"\"\"\n",
    "  def __init__(self, input_size, random_state=SEED,\n",
    "               track_rec=False, lam=10, lam_decay=1, n_neighbors=15):\n",
    "    super().__init__(input_size=input_size,\n",
    "                      random_state=random_state,\n",
    "                      embedder=umap.UMAP,\n",
    "                      track_rec=track_rec,\n",
    "                      lam=lam,\n",
    "                      n_neighbors=n_neighbors)\n",
    "\n",
    "\n",
    "class GRAE_TSNE(ManifoldNet):\n",
    "  \"\"\"t-SNE GRAE, as presented in the supplement.\"\"\"\n",
    "  def __init__(self, input_size, random_state=SEED, track_rec=False,\n",
    "               lam=10, lam_decay=1, perplexity=30):\n",
    "    super().__init__(input_size=input_size,\n",
    "                      random_state=random_state,\n",
    "                      embedder=TSNE,\n",
    "                      track_rec=track_rec,\n",
    "                      lam=lam,\n",
    "                      n_jobs=-1,\n",
    "                      verbose=0,\n",
    "                      perplexity=perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsupervised Metrics\n",
    "# Code adapted from the Topological autoencoders paper\n",
    "class MeasureCalculator():\n",
    "    # measures = MeasureRegistrator()\n",
    "\n",
    "    def __init__(self, X, Z, X_hat, k_max=20):\n",
    "        self.k_max = k_max\n",
    "        self.X = X\n",
    "        self.X_hat = X_hat\n",
    "        self.pairwise_X = squareform(pdist(X))\n",
    "        self.pairwise_Z = squareform(pdist(Z))\n",
    "        self.neighbours_X, self.ranks_X = \\\n",
    "            self._neighbours_and_ranks(self.pairwise_X, k_max)\n",
    "        self.neighbours_Z, self.ranks_Z = \\\n",
    "            self._neighbours_and_ranks(self.pairwise_Z, k_max)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _neighbours_and_ranks(distances, k):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - distances,        distance matrix [n times n],\n",
    "        - k,                number of nearest neighbours to consider\n",
    "        Returns:\n",
    "        - neighbourhood,    contains the sample indices (from 0 to n-1) of kth nearest neighbor of current sample [n times k]\n",
    "        - ranks,            contains the rank of each sample to each sample [n times n], whereas entry (i,j) gives the rank that sample j has to i (the how many 'closest' neighbour j is to i)\n",
    "        \"\"\"\n",
    "        # Warning: this is only the ordering of neighbours that we need to\n",
    "        # extract neighbourhoods below. The ranking comes later!\n",
    "        indices = np.argsort(distances, axis=-1, kind='stable')\n",
    "\n",
    "        # Extract neighbourhoods.\n",
    "        neighbourhood = indices[:, 1:k+1]\n",
    "\n",
    "        # Convert this into ranks (finally)\n",
    "        ranks = indices.argsort(axis=-1, kind='stable')\n",
    "\n",
    "        return neighbourhood, ranks\n",
    "\n",
    "    def get_X_neighbours_and_ranks(self, k):\n",
    "        return self.neighbours_X[:, :k], self.ranks_X\n",
    "\n",
    "    def get_Z_neighbours_and_ranks(self, k):\n",
    "        return self.neighbours_Z[:, :k], self.ranks_Z\n",
    "\n",
    "    def compute_k_independent_measures(self):\n",
    "        return {key: fn(self) for key, fn in\n",
    "                self.measures.get_k_independent_measures().items()}\n",
    "\n",
    "    def compute_k_dependent_measures(self, k):\n",
    "        return {key: fn(self, k) for key, fn in\n",
    "                self.measures.get_k_dependent_measures().items()}\n",
    "\n",
    "    def compute_measures_for_ks(self, ks):\n",
    "        return {\n",
    "            key: np.array([fn(self, k) for k in ks])\n",
    "            for key, fn in self.measures.get_k_dependent_measures().items()\n",
    "        }\n",
    "\n",
    "\n",
    "    # @measures.register(False)\n",
    "    def stress(self):\n",
    "        sum_of_squared_differences = \\\n",
    "            np.square(self.pairwise_X - self.pairwise_Z).sum()\n",
    "        sum_of_squares = np.square(self.pairwise_Z).sum()\n",
    "\n",
    "        return np.sqrt(sum_of_squared_differences / sum_of_squares)\n",
    "\n",
    "    # @measures.register(False)\n",
    "    def rmse(self):\n",
    "        n = self.pairwise_X.shape[0]\n",
    "        sum_of_squared_differences = np.square(\n",
    "            self.pairwise_X - self.pairwise_Z).sum()\n",
    "        return np.sqrt(sum_of_squared_differences / n**2)\n",
    "\n",
    "    @staticmethod\n",
    "    def _trustworthiness(X_neighbourhood, X_ranks, Z_neighbourhood,\n",
    "                         Z_ranks, n, k):\n",
    "        '''\n",
    "        Calculates the trustworthiness measure between the data space `X`\n",
    "        and the latent space `Z`, given a neighbourhood parameter `k` for\n",
    "        defining the extent of neighbourhoods.\n",
    "        '''\n",
    "\n",
    "        result = 0.0\n",
    "\n",
    "        # Calculate number of neighbours that are in the $k$-neighbourhood\n",
    "        # of the latent space but not in the $k$-neighbourhood of the data\n",
    "        # space.\n",
    "        for row in range(X_ranks.shape[0]):\n",
    "            missing_neighbours = np.setdiff1d(\n",
    "                Z_neighbourhood[row],\n",
    "                X_neighbourhood[row]\n",
    "            )\n",
    "\n",
    "            for neighbour in missing_neighbours:\n",
    "                result += (X_ranks[row, neighbour] - k)\n",
    "\n",
    "        return 1 - 2 / (n * k * (2 * n - 3 * k - 1) ) * result\n",
    "\n",
    "    # @measures.register(True)\n",
    "    def trustworthiness(self, k):\n",
    "        X_neighbourhood, X_ranks = self.get_X_neighbours_and_ranks(k)\n",
    "        Z_neighbourhood, Z_ranks = self.get_Z_neighbours_and_ranks(k)\n",
    "        n = self.pairwise_X.shape[0]\n",
    "        return self._trustworthiness(X_neighbourhood, X_ranks, Z_neighbourhood,\n",
    "                                     Z_ranks, n, k)\n",
    "\n",
    "    # @measures.register(True)\n",
    "    def continuity(self, k):\n",
    "        '''\n",
    "        Calculates the continuity measure between the data space `X` and the\n",
    "        latent space `Z`, given a neighbourhood parameter `k` for setting up\n",
    "        the extent of neighbourhoods.\n",
    "\n",
    "        This is just the 'flipped' variant of the 'trustworthiness' measure.\n",
    "        '''\n",
    "\n",
    "        X_neighbourhood, X_ranks = self.get_X_neighbours_and_ranks(k)\n",
    "        Z_neighbourhood, Z_ranks = self.get_Z_neighbours_and_ranks(k)\n",
    "        n = self.pairwise_X.shape[0]\n",
    "        # Notice that the parameters have to be flipped here.\n",
    "        return self._trustworthiness(Z_neighbourhood, Z_ranks, X_neighbourhood,\n",
    "                                     X_ranks, n, k)\n",
    "\n",
    "    # @measures.register(True)\n",
    "    def neighbourhood_loss(self, k):\n",
    "        '''\n",
    "        Calculates the neighbourhood loss quality measure between the data\n",
    "        space `X` and the latent space `Z` for some neighbourhood size $k$\n",
    "        that has to be pre-defined.\n",
    "        '''\n",
    "\n",
    "        X_neighbourhood, _ = self.get_X_neighbours_and_ranks(k)\n",
    "        Z_neighbourhood, _ = self.get_Z_neighbours_and_ranks(k)\n",
    "\n",
    "        result = 0.0\n",
    "        n = self.pairwise_X.shape[0]\n",
    "\n",
    "        for row in range(n):\n",
    "            shared_neighbours = np.intersect1d(\n",
    "                X_neighbourhood[row],\n",
    "                Z_neighbourhood[row],\n",
    "                assume_unique=True\n",
    "            )\n",
    "\n",
    "            result += len(shared_neighbours) / k\n",
    "\n",
    "        return 1.0 - result / n\n",
    "\n",
    "\n",
    "    # @measures.register(True)\n",
    "    def rank_correlation(self, k):\n",
    "        '''\n",
    "        Calculates the spearman rank correlation of the data\n",
    "        space `X` with respect to the latent space `Z`, subject to its $k$\n",
    "        nearest neighbours.\n",
    "        '''\n",
    "\n",
    "        X_neighbourhood, X_ranks = self.get_X_neighbours_and_ranks(k)\n",
    "        Z_neighbourhood, Z_ranks = self.get_Z_neighbours_and_ranks(k)\n",
    "\n",
    "        n = self.pairwise_X.shape[0]\n",
    "        #we gather\n",
    "        gathered_ranks_x = []\n",
    "        gathered_ranks_z = []\n",
    "        for row in range(n):\n",
    "            #we go from X to Z here:\n",
    "            for neighbour in X_neighbourhood[row]:\n",
    "                rx = X_ranks[row, neighbour]\n",
    "                rz = Z_ranks[row, neighbour]\n",
    "                gathered_ranks_x.append(rx)\n",
    "                gathered_ranks_z.append(rz)\n",
    "        rs_x = np.array(gathered_ranks_x)\n",
    "        rs_z = np.array(gathered_ranks_z)\n",
    "        coeff, _ = spearmanr(rs_x, rs_z)\n",
    "\n",
    "        ##use only off-diagonal (non-trivial) ranks:\n",
    "        #inds = ~np.eye(X_ranks.shape[0],dtype=bool)\n",
    "        #coeff, pval = spearmanr(X_ranks[inds], Z_ranks[inds])\n",
    "        return coeff\n",
    "\n",
    "    # @measures.register(True)\n",
    "    def mrre(self, k):\n",
    "        '''\n",
    "        Calculates the mean relative rank error quality metric of the data\n",
    "        space `X` with respect to the latent space `Z`, subject to its $k$\n",
    "        nearest neighbours.\n",
    "        '''\n",
    "\n",
    "        X_neighbourhood, X_ranks = self.get_X_neighbours_and_ranks(k)\n",
    "        Z_neighbourhood, Z_ranks = self.get_Z_neighbours_and_ranks(k)\n",
    "\n",
    "        n = self.pairwise_X.shape[0]\n",
    "\n",
    "        # First component goes from the latent space to the data space, i.e.\n",
    "        # the relative quality of neighbours in `Z`.\n",
    "\n",
    "        mrre_ZX = 0.0\n",
    "        for row in range(n):\n",
    "            for neighbour in Z_neighbourhood[row]:\n",
    "                rx = X_ranks[row, neighbour]\n",
    "                rz = Z_ranks[row, neighbour]\n",
    "\n",
    "                mrre_ZX += abs(rx - rz) / rz\n",
    "\n",
    "        # Second component goes from the data space to the latent space,\n",
    "        # i.e. the relative quality of neighbours in `X`.\n",
    "\n",
    "        mrre_XZ = 0.0\n",
    "        for row in range(n):\n",
    "            # Note that this uses a different neighbourhood definition!\n",
    "            for neighbour in X_neighbourhood[row]:\n",
    "                rx = X_ranks[row, neighbour]\n",
    "                rz = Z_ranks[row, neighbour]\n",
    "\n",
    "                # Note that this uses a different normalisation factor\n",
    "                mrre_XZ += abs(rx - rz) / rx\n",
    "\n",
    "        # Normalisation constant\n",
    "        C = n * sum([abs(2*j - n - 1) / j for j in range(1, k+1)])\n",
    "        # return mrre_ZX / C, mrre_XZ / C\n",
    "        return mrre_ZX / C\n",
    "\n",
    "    # @measures.register(False)\n",
    "    def density_global(self, sigma=0.1):\n",
    "        X = self.pairwise_X\n",
    "        X = X / X.max()\n",
    "        Z = self.pairwise_Z\n",
    "        Z = Z / Z.max()\n",
    "\n",
    "        density_x = np.sum(np.exp(-(X ** 2) / sigma), axis=-1)\n",
    "        density_x /= density_x.sum(axis=-1)\n",
    "\n",
    "        density_z = np.sum(np.exp(-(Z ** 2) / sigma), axis=-1)\n",
    "        density_z /= density_z.sum(axis=-1)\n",
    "\n",
    "        return np.abs(density_x - density_z).sum()\n",
    "\n",
    "    # @measures.register(False)\n",
    "    def density_kl_global(self, sigma=0.1):\n",
    "        X = self.pairwise_X\n",
    "        X = X / X.max()\n",
    "        Z = self.pairwise_Z\n",
    "        Z = Z / Z.max()\n",
    "\n",
    "        density_x = np.sum(np.exp(-(X ** 2) / sigma), axis=-1)\n",
    "        density_x /= density_x.sum(axis=-1)\n",
    "\n",
    "        density_z = np.sum(np.exp(-(Z ** 2) / sigma), axis=-1)\n",
    "        density_z /= density_z.sum(axis=-1)\n",
    "\n",
    "        return (density_x * (np.log(density_x) - np.log(density_z))).sum()\n",
    "\n",
    "    # @measures.register(False)\n",
    "    def density_kl_global_10(self):\n",
    "        return self.density_kl_global(10.)\n",
    "\n",
    "    # @measures.register(False)\n",
    "    def density_kl_global_1(self):\n",
    "        return self.density_kl_global(1.)\n",
    "\n",
    "    # @measures.register(False)\n",
    "    def density_kl_global_01(self):\n",
    "        return self.density_kl_global(0.1)\n",
    "\n",
    "    # @measures.register(False)\n",
    "    def density_kl_global_001(self):\n",
    "        return self.density_kl_global(0.01)\n",
    "\n",
    "    # @measures.register(False)\n",
    "    def density_kl_global_0001(self):\n",
    "        return self.density_kl_global(0.001)\n",
    "\n",
    "    def reconstruction(self):\n",
    "      if self.X_hat == None:\n",
    "        return None\n",
    "\n",
    "      return mean_squared_error(self.X, self.X_hat)\n",
    "\n",
    "\n",
    "    def get_metrics(self, metrics):\n",
    "      results = dict()\n",
    "\n",
    "      for metric in metrics:\n",
    "        args = metric.split('_')\n",
    "\n",
    "        if len(args) == 2:\n",
    "          m, k = args\n",
    "          k = dict(k=int(k))\n",
    "        elif len(args) == 1:\n",
    "          m, k = args[0], dict()\n",
    "        else:\n",
    "          raise Exception('Invalid string metric.')\n",
    "\n",
    "        results[metric]=getattr(self, m)(**k)\n",
    "\n",
    "      return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dict with various model parameters\n",
    "\n",
    "# Neighborhood parameters of manifold learners\n",
    "PHATE_knn = dict(\n",
    "    Faces=dict(knn=5),\n",
    "    RotatedDigits=dict(knn=5),\n",
    "    ribbons=dict(knn=20),\n",
    "    Embryoid=dict(knn=5),\n",
    ")\n",
    "\n",
    "UMAP_n_neighbors = dict(\n",
    "    Faces=dict(n_neighbors=15),\n",
    "    RotatedDigits=dict(n_neighbors=15),\n",
    "    ribbons=dict(n_neighbors=20),\n",
    "    Embryoid=dict(n_neighbors=15),\n",
    ")\n",
    "\n",
    "TSNE_perp = dict(\n",
    "    Faces=dict(perplexity=10),\n",
    "    RotatedDigits=dict(perplexity=10),\n",
    "    ribbons=dict(perplexity=30),\n",
    "    Embryoid=dict(perplexity=10),\n",
    ")\n",
    "\n",
    "# t parameter for PHATE\n",
    "t = 'auto'\n",
    "\n",
    "ds_names = dataset_constructors.keys()\n",
    "\n",
    "\n",
    "# Input size. Should be passed to AE and GRAE to adjust\n",
    "# input and output layers accordingly\n",
    "size_dict = dict(\n",
    "              RotatedDigits=dict(input_size=784),\n",
    "              ribbons=dict(input_size=3),\n",
    "              Faces=dict(input_size=4096),\n",
    "              Embryoid=dict(input_size=EB_COMPONENTS),\n",
    ")\n",
    "\n",
    "# Build dict with both input size and knn parameter\n",
    "PHATE_knn_size = copy.deepcopy(PHATE_knn)\n",
    "\n",
    "for key, item in PHATE_knn_size.items():\n",
    "  item.update(size_dict[key])\n",
    "\n",
    "UMAP_n_neighbors_size = copy.deepcopy(UMAP_n_neighbors)\n",
    "\n",
    "for key, item in UMAP_n_neighbors_size.items():\n",
    "  item.update(size_dict[key])\n",
    "\n",
    "TSNE_perp_size = copy.deepcopy(TSNE_perp)\n",
    "\n",
    "for key, item in TSNE_perp_size.items():\n",
    "  item.update(size_dict[key])\n",
    "\n",
    "\n",
    "# As placeholder when no parameters are needed for dataset specific inits\n",
    "empty_dict = dict(zip(ds_names, [{} for _ in ds_names]))\n",
    "\n",
    "\n",
    "# Parameter variable\n",
    "params = {\n",
    "    'Umap': dict( # Vanilla Umap with no transforms, use Umap_t instead\n",
    "        constructor=umap.UMAP,\n",
    "        phate_cache=False,\n",
    "        numpy=True,\n",
    "        train_only=True,\n",
    "        init_default=dict(), # Parameters to be used for all model inits\n",
    "        init_dataset=empty_dict, # Dataset specific model inits\n",
    "        FIT_DEFAULT={}, # Parameters to be used for all model fits\n",
    "        fit_dataset=empty_dict # Dataset specific model fits\n",
    "        ),\n",
    "\n",
    "      'Umap_t': dict(\n",
    "        constructor=umap.UMAP,\n",
    "        phate_cache=False,\n",
    "        numpy=True,\n",
    "        train_only=False,\n",
    "        init_default=dict(),\n",
    "        init_dataset=UMAP_n_neighbors,\n",
    "        FIT_DEFAULT={},\n",
    "        fit_dataset=empty_dict\n",
    "        ),\n",
    "\n",
    "\n",
    "      'PHATE': dict( # vanilla PHATE\n",
    "        constructor=phate.PHATE,\n",
    "        phate_cache=True,\n",
    "        numpy=True,\n",
    "        train_only=True,\n",
    "        init_default=dict(verbose=0, n_jobs=-1, t=t),\n",
    "        init_dataset=PHATE_knn,\n",
    "        FIT_DEFAULT={},\n",
    "        fit_dataset=empty_dict\n",
    "        ),\n",
    "\n",
    "      'TSNE': dict(\n",
    "        constructor=TSNE,\n",
    "        phate_cache=False,\n",
    "        numpy=True,\n",
    "        train_only=True,\n",
    "        init_default=dict(verbose=0, n_jobs=-1),\n",
    "        init_dataset=empty_dict,\n",
    "        FIT_DEFAULT={},\n",
    "        fit_dataset=empty_dict\n",
    "        ),\n",
    "\n",
    "\n",
    "      'GRAE': dict( # This is GRAE.\n",
    "        constructor=GRAE,\n",
    "        phate_cache=True,\n",
    "        numpy=False,\n",
    "        train_only=False,\n",
    "        init_default=dict(lam=1, t=t),\n",
    "        init_dataset=PHATE_knn_size,\n",
    "        FIT_DEFAULT={},\n",
    "        fit_dataset=empty_dict\n",
    "        ),\n",
    "      'GRAE_UMAP': dict(\n",
    "        constructor=GRAE_UMAP,\n",
    "        phate_cache=False,\n",
    "        numpy=False,\n",
    "        train_only=False,\n",
    "        init_default=dict(lam=1),\n",
    "        init_dataset=UMAP_n_neighbors_size,\n",
    "        FIT_DEFAULT={},\n",
    "        fit_dataset=empty_dict\n",
    "      ),\n",
    "      'GRAE_TSNE': dict(\n",
    "        constructor=GRAE_TSNE,\n",
    "        phate_cache=False,\n",
    "        numpy=False,\n",
    "        train_only=False,\n",
    "        init_default=dict(lam=1),\n",
    "        init_dataset=TSNE_perp_size,\n",
    "        FIT_DEFAULT={},\n",
    "        fit_dataset=empty_dict\n",
    "      ),\n",
    "      'AE': dict(\n",
    "        constructor=AE,\n",
    "        phate_cache=False,\n",
    "        numpy=False,\n",
    "        train_only=False,\n",
    "        init_default={},\n",
    "        init_dataset=size_dict,\n",
    "        FIT_DEFAULT={},\n",
    "        fit_dataset=empty_dict\n",
    "        ),\n",
    "}\n",
    "\n",
    "tae = False\n",
    "\n",
    "try:\n",
    "  params.update({\n",
    "      'TopoAE': dict(\n",
    "        constructor=TopoAE,\n",
    "        phate_cache=False,\n",
    "        numpy=False,\n",
    "        train_only=False,\n",
    "        init_default={},\n",
    "        init_dataset=size_dict,\n",
    "        FIT_DEFAULT={},\n",
    "        fit_dataset=empty_dict\n",
    "      )\n",
    "  })\n",
    "  tae = True\n",
    "except Exception:\n",
    "  print('TopoAE is not defined. If you wish to use them, please refer to the Topological autoencoders subsection.')\n",
    "\n",
    "\n",
    "# Add variants of PHATE-Net, UMAP-Net and TSNE-Net\n",
    "for n in (1, 10, 50, 100, 200, 1000, 10000):\n",
    "  params[f'GRAE_{n}'] = copy.deepcopy(params['GRAE'])\n",
    "  params[f'GRAE_{n}']['init_default']['lam'] = n\n",
    "  params[f'GRAE_UMAP_{n}'] = copy.deepcopy(params['GRAE_UMAP'])\n",
    "  params[f'GRAE_UMAP_{n}']['init_default']['lam'] = n\n",
    "  params[f'GRAE_TSNE_{n}'] = copy.deepcopy(params['GRAE_TSNE'])\n",
    "  params[f'GRAE_TSNE_{n}']['init_default']['lam'] = n\n",
    "  if tae:\n",
    "    params[f'TopoAE_{n}'] = copy.deepcopy(params['TopoAE'])\n",
    "    params[f'TopoAE_{n}']['init_default']['lam'] = n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit models\n",
    "# All embeddings are saved under the embeddings folder\n",
    "\n",
    "# Remove previous embedding folder if any\n",
    "# !rm -rf 'embeddings'\n",
    "\n",
    "if os.path.exists('embeddings'):\n",
    "    shutil.rmtree('embeddings')\n",
    "\n",
    "os.mkdir('embeddings')\n",
    "\n",
    "# Experiment loop\n",
    "for model in MODELS:\n",
    "  print(f'Training {model}...')\n",
    "\n",
    "  os.mkdir(os.path.join('embeddings', model))\n",
    "\n",
    "  for i, dataset in enumerate(DATASETS):\n",
    "    target = os.path.join('embeddings', model, dataset)\n",
    "\n",
    "    os.mkdir(target)\n",
    "\n",
    "    print(f'   On {dataset}...')\n",
    "    # Training loop\n",
    "    for j in range(RUNS):\n",
    "      print(f'       Run {j + 1}...')\n",
    "\n",
    "      # Fetch and split dataset. Handle numpy input for some models\n",
    "      data_train = dataset_constructors[dataset](split=\"train\",\n",
    "                                                 split_ratio=FIT_RATIO,\n",
    "                                                 seed=SEED)\n",
    "      data_test = dataset_constructors[dataset](split=\"test\",\n",
    "                                                split_ratio=FIT_RATIO,\n",
    "                                                seed=SEED)\n",
    "\n",
    "\n",
    "      data_train_np, y_train = data_train.numpy()\n",
    "      data_test_np, y_test = data_test.numpy()\n",
    "\n",
    "\n",
    "      if params[model]['numpy']:\n",
    "        data_train = data_train_np\n",
    "        data_test = data_test_np\n",
    "\n",
    "\n",
    "\n",
    "      m = params[model]['constructor']( # New Model\n",
    "          **params[model]['init_default'],\n",
    "          **params[model]['init_dataset'][dataset],\n",
    "          random_state=RANDOM_STATES[j])\n",
    "\n",
    "\n",
    "      # Benchmark fit time\n",
    "      fit_start = time.time()\n",
    "\n",
    "      z_train = m.fit_transform(data_train,\n",
    "            **params[model]['FIT_DEFAULT'],\n",
    "            **params[model]['fit_dataset'][dataset])\n",
    "\n",
    "      fit_stop = time.time()\n",
    "\n",
    "      fit_time = fit_stop - fit_start\n",
    "\n",
    "\n",
    "      if not params[model]['train_only']:\n",
    "        # Benchmark transform time if required\n",
    "        transform_start = time.time()\n",
    "        z_test = m.transform(data_test)\n",
    "        transform_stop = time.time()\n",
    "\n",
    "        transform_time = transform_stop - transform_start\n",
    "\n",
    "\n",
    "\n",
    "      if params[model]['train_only']:\n",
    "         # T-SNE and PHATE do not have inverse transforms\n",
    "        inv_train, inv_test, rec_train, rec_test = None, None, None, None\n",
    "      else:\n",
    "        inv_train = m.inverse_transform(z_train)\n",
    "        inv_test = m.inverse_transform(z_test)\n",
    "\n",
    "        rec_train = mean_squared_error(data_train_np, inv_train)\n",
    "        rec_test = mean_squared_error(data_test_np, inv_test)\n",
    "\n",
    "\n",
    "      # Save embeddings\n",
    "      if params[model]['train_only']:\n",
    "        obj = dict(z_train=z_train, z_test=None,\n",
    "                   rec_train=None, rec_test=None,\n",
    "                   fit_time=fit_time, transform_time=None,\n",
    "                   dataset_seed=SEED, run_seed=RANDOM_STATES[j])\n",
    "      else:\n",
    "        obj = dict(z_train=z_train, z_test=z_test,\n",
    "                   rec_train=rec_train, rec_test=rec_test,\n",
    "                   fit_time=fit_time, transform_time=transform_time,\n",
    "                   dataset_seed=SEED, run_seed=RANDOM_STATES[j])\n",
    "\n",
    "\n",
    "      save_dict(obj, os.path.join(target, f'run_{j + 1}.pkl'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prettier names for models and datasets\n",
    "\n",
    "model_name = dict(Umap_t= 'UMAP',\n",
    "                  Umap='UMAP',\n",
    "                  diffusion_net='Diffusion Nets',\n",
    "                  PHATE='PHATE',\n",
    "                  TSNE='t-SNE',\n",
    "                  AE='Autoencoder')\n",
    "\n",
    "base_name = dict(GRAE='GRAE (', GRAE_TSNE='GRAE t-SNE (',\n",
    "                 GRAE_UMAP='GRAE UMAP (', TopoAE='TAE (')\n",
    "\n",
    "for m in MODELS:\n",
    "  if m not in model_name:\n",
    "    splits = m.split('_')\n",
    "    base, lam = splits[0], splits[1]\n",
    "    model_name[m] = base_name[base] + f'{lam})'\n",
    "\n",
    "\n",
    "\n",
    "ds_name = dict(ribbons='Swiss Roll',\n",
    "               Faces='Faces',\n",
    "               Embryoid='Embryoid',\n",
    "               RotatedDigits='Rotated Digits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot embeddings\n",
    "PLOT_RUN = 1\n",
    "\n",
    "\n",
    "titles = [model_name[m] for m in MODELS]\n",
    "n_d = len(DATASETS)\n",
    "n_m = len(MODELS)\n",
    "fig, ax = plt.subplots(n_d, n_m, figsize=(n_m * 3.5, n_d * 3.5))\n",
    "\n",
    "for j, model in enumerate(MODELS):\n",
    "  for i, dataset in enumerate(DATASETS):\n",
    "    file_path = os.path.join('embeddings', model, dataset, f'run_{PLOT_RUN}.pkl')\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "      # Retrieve datasets for coloring\n",
    "      data = load_dict(file_path)\n",
    "      X_train = dataset_constructors[dataset](split='train',\n",
    "                                              seed=data['dataset_seed'])\n",
    "      X_test = dataset_constructors[dataset](split='test',\n",
    "                                             seed=data['dataset_seed'])\n",
    "      _, y_train = X_train.numpy()\n",
    "      _, y_test = X_test.numpy()\n",
    "      z_train, z_test = data['z_train'], data['z_test']\n",
    "    else:\n",
    "      # Filler if plot is not found\n",
    "      z_train, z_test = np.array([[0, 0]]), np.array([[0, 0]])\n",
    "      y_train = np.array([1])\n",
    "      y_test = np.array([1])\n",
    "\n",
    "\n",
    "    if n_d == 1:\n",
    "      ax_i = ax[j]\n",
    "    elif n_m == 1:\n",
    "      ax_i = ax[i]\n",
    "    else:\n",
    "      ax_i = ax[i, j]\n",
    "\n",
    "    l = ax_i.scatter(*z_train.T, s = 1.5,  alpha=.2, color='grey')\n",
    "\n",
    "    ax_i.scatter(*z_test.T, c = y_test, s = 15, cmap='jet')\n",
    "\n",
    "\n",
    "\n",
    "    if i == 0:\n",
    "      ax_i.set_title(f'{titles[j]}', fontsize=20, color='black')\n",
    "    ax_i.set_xticks([])\n",
    "    ax_i.set_yticks([])\n",
    "\n",
    "\n",
    "plt.savefig(os.path.join('results', 'plot.png'))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
